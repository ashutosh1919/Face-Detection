{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\group.jpeg',cv2.IMREAD_COLOR)\n",
    "\n",
    "# Create a window holder to show you image in.\n",
    "#cv2.namedWindow(\"Image Recognition Project\",cv2.WINDOW_NORMAL)\n",
    "#cv2.imshow(\"Image Recognition Project\",image)\n",
    "\n",
    "#plt.imshow(image)\n",
    "#plt.show()\n",
    "#print(type(image))  //ndarray type.\n",
    "\n",
    "# Both plt and cv2 figures are different.\n",
    "# Because cv2 uses BGR format and plt uses RGB format.\n",
    "\n",
    "# Press any key to close the external window.\n",
    "#cv2.waitKey()\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "# For writing and reading images\n",
    "#cv2.imwrite(file_path(str),image(np.ndarray))\n",
    "#cv2.imread(file_path(str),read_mode(int))\n",
    "\n",
    "#cv2.imwrite('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\cv2image.png',image)\n",
    "#cv2.imwrite('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\pltimage.png',image)\n",
    "\n",
    "# Loading the video.\n",
    "video = cv2.VideoCapture('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\beanv.mp4')\n",
    "#cv2.namedWindow('Face Detection Project',cv2.WINDOW_NORMAL)\n",
    "\n",
    "# to run video using OpenCV\n",
    "#while True:\n",
    "#    ret,frame = video.read()\n",
    "#    if not ret:\n",
    "#        break\n",
    "#    cv2.imshow('Face Detection Project',frame)\n",
    "\n",
    "    # Code 27 is of Esc key\n",
    "#    if cv2.waitKey(20) and 0xFF == 27:\n",
    "#        break\n",
    "\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "# to run video using plt.\n",
    "#try:\n",
    "#    while True:\n",
    "#        _,frame = video.read()\n",
    "#        plt.show(frame)\n",
    "#        clear_output(wait=True)\n",
    "#except KeyboardInterrupt:\n",
    "#    print(\"Live Video Interrupted.\")\n",
    "\n",
    "\n",
    "# Here starts the algorithm for training the model.\n",
    "# We will use CascadeClassifier.\n",
    "\n",
    "# Example code for face detection in an Image.\n",
    "#detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "#scale_factor = 1.2\n",
    "#min_neighbors = 5\n",
    "\n",
    "#min_size = (30,30)\n",
    "#biggest_only = True\n",
    "\n",
    "#flags = cv2.CASCADE_FIND_BIGGEST_OBJECT | cv2.CASCADE_DO_ROUGH_SEARCH if biggest_only else cv2.CASCADE_SCALE_IMAGE\n",
    "\n",
    "#faces_coord = detector.detectMultiScale(image,scaleFactor=scale_factor,minNeighbors=min_neighbors,minSize=min_size,flags=flags)\n",
    "\n",
    "#print(\"Type : \",str(type(faces_coord)))\n",
    "#print(faces_coord)\n",
    "#print(\"Length : \",str(len(faces_coord)))\n",
    "\n",
    "#for (x,y,w,h) in faces_coord:\n",
    "#    cv2.rectangle(image,(x,y),(x+w,y+h),(150,150,0),8)\n",
    "\n",
    "#cv2.namedWindow(\"Face Detected Image\",cv2.WINDOW_NORMAL)\n",
    "#cv2.imshow(\"Face Detected Image\",image)\n",
    "#plt.title(\"Face Detected Image\")\n",
    "#plt.axis('off')\n",
    "#plt.imshow(image)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetector(object):\n",
    "    def __init__(self,xml_path):\n",
    "        self.classifier = cv2.CascadeClassifier(xml_path)\n",
    "\n",
    "    def detect(self,image,biggest_only=True):\n",
    "        scale_factor = 1.2\n",
    "        min_neighbours = 5\n",
    "        min_size = (30,30)\n",
    "        biggest_only = True\n",
    "        flags = cv2.CASCADE_FIND_BIGGEST_OBJECT | cv2.CASCADE_DO_ROUGH_SEARCH if biggest_only else cv2.CASCADE_SCALE_IMAGE\n",
    "        faces_coord = self.classifier.detectMultiScale(image,scaleFactor=scale_factor,minNeighbors=min_neighbours,minSize=min_size,flags=flags)\n",
    "        return faces_coord\n",
    "\n",
    "class VideoCamera(object):\n",
    "    def __init__(self,index=0):\n",
    "        self.video = cv2.VideoCapture('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\samplev.mp4')\n",
    "        self.index = index\n",
    "\n",
    "    def get_frame(self,in_grayscale=False):\n",
    "        _,frame = self.video.read()\n",
    "        if in_grayscale:\n",
    "            frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        return frame\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live Video Interrupted by Keyboard.\n"
     ]
    }
   ],
   "source": [
    "video1 = VideoCamera()\n",
    "detector = FaceDetector(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frame = video1.get_frame()\n",
    "        #if not ret:\n",
    "        #    break\n",
    "        faces_coord = detector.detect(frame)\n",
    "        for (x,y,w,h) in faces_coord:\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(150,150,0),8)\n",
    "        plt.title('Face Detection in video')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        clear_output(wait=True)\n",
    "        #cv2.imshow('Video Face Detection',frame)\n",
    "        #if cv2.waitKey(20) and 0xFF == 27:\n",
    "        #    break\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Live Video Interrupted by Keyboard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting the face of the person.\n",
    "# 70% of the face will be visible.\n",
    "def cut_faces(image,faces_coord):\n",
    "    faces = []\n",
    "    for (x,y,w,h) in faces_coord:\n",
    "        w_rm = int(0.2*w/2)\n",
    "        faces.append(image[y:y+h,x+w_rm:x+w-w_rm])\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live Video Interrupted by Keyboard.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x226e8a699e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        frame = video1.get_frame()\n",
    "        #if not ret:\n",
    "        #    break\n",
    "        faces_coord = detector.detect(frame)\n",
    "        if(len(faces_coord)):\n",
    "            faces = cut_faces(frame,faces_coord)\n",
    "            plt.title('Face Detection in video')\n",
    "            plt.axis('off')\n",
    "            plt.imshow(faces[0])\n",
    "            plt.show()\n",
    "            clear_output(wait=True)\n",
    "        #cv2.imshow('Video Face Detection',frame)\n",
    "        #if cv2.waitKey(20) and 0xFF == 27:\n",
    "        #    break\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Live Video Interrupted by Keyboard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_intensity(images):\n",
    "    images_norm = []\n",
    "    for image in images:\n",
    "        is_color = len(image.shape)==3\n",
    "        if is_color:\n",
    "            image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "        images_norm.append(cv2.equalizeHist(image))\n",
    "    return images_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live Video Interrupted by Keyboard.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        frame = video1.get_frame()\n",
    "        #if not ret:\n",
    "        #    break\n",
    "        faces_coord = detector.detect(frame)\n",
    "        if(len(faces_coord)):\n",
    "            faces = cut_faces(frame,faces_coord)\n",
    "            faces = normalize_intensity(faces)\n",
    "            plt.title('Face Detection in video')\n",
    "            plt.axis('off')\n",
    "            plt.imshow(faces[0])\n",
    "            plt.show()\n",
    "            clear_output(wait=True)\n",
    "        #cv2.imshow('Video Face Detection',frame)\n",
    "        #if cv2.waitKey(20) and 0xFF == 27:\n",
    "        #    break\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Live Video Interrupted by Keyboard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(images,size=(50,50)):\n",
    "    images_norm = []\n",
    "    for image in images:\n",
    "        if image.shape < size:\n",
    "            image_norm = cv2.resize(image,size,interpolation=cv2.INTER_AREA)\n",
    "        else:\n",
    "            image_norm = cv2.resize(image,size,interpolation=cv2.INTER_CUBIC)\n",
    "        images_norm.append(image_norm)\n",
    "    return images_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live Video Interrupted by Keyboard.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x226e8943748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        frame = video1.get_frame()\n",
    "        #if not ret:\n",
    "        #    break\n",
    "        faces_coord = detector.detect(frame)\n",
    "        if(len(faces_coord)):\n",
    "            faces = cut_faces(frame,faces_coord)\n",
    "            faces = normalize_intensity(faces)\n",
    "            faces = resize(faces)\n",
    "            plt.title('Face Detection in video')\n",
    "            plt.axis('off')\n",
    "            plt.imshow(faces[0])\n",
    "            plt.show()\n",
    "            clear_output(wait=True)\n",
    "        #cv2.imshow('Video Face Detection',frame)\n",
    "        #if cv2.waitKey(20) and 0xFF == 27:\n",
    "        #    break\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Live Video Interrupted by Keyboard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person : ashu\n"
     ]
    }
   ],
   "source": [
    "folder = \"people/\"+input(\"Person : \").lower()\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "    \n",
    "    image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\ashu.jpg',cv2.IMREAD_COLOR)\n",
    "    faces_coord = detector.detect(image)\n",
    "    if(len(faces_coord)):\n",
    "        faces = cut_faces(image,faces_coord)\n",
    "        faces = normalize_intensity(faces)\n",
    "        faces = resize(faces)\n",
    "        cv2.imwrite(folder+'/ashu_norm.jpg',faces[0])\n",
    "        \n",
    "    image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\ashu2.jpg',cv2.IMREAD_COLOR)\n",
    "    faces_coord = detector.detect(image)\n",
    "    if(len(faces_coord)):\n",
    "        faces = cut_faces(image,faces_coord)\n",
    "        faces = normalize_intensity(faces)\n",
    "        faces = resize(faces)\n",
    "        cv2.imwrite(folder+'/ashu2_norm.jpg',faces[0])\n",
    "        \n",
    "    image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\ashu3.jpg',cv2.IMREAD_COLOR)\n",
    "    faces_coord = detector.detect(image)\n",
    "    if(len(faces_coord)):\n",
    "        faces = cut_faces(image,faces_coord)\n",
    "        faces = normalize_intensity(faces)\n",
    "        faces = resize(faces)\n",
    "        cv2.imwrite(folder+'/ashu3_norm.jpg',faces[0])\n",
    "        \n",
    "    image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\ashu4.jpg',cv2.IMREAD_COLOR)\n",
    "    faces_coord = detector.detect(image)\n",
    "    if(len(faces_coord)):\n",
    "        faces = cut_faces(image,faces_coord)\n",
    "        faces = normalize_intensity(faces)\n",
    "        faces = resize(faces)\n",
    "        cv2.imwrite(folder+'/ashu4_norm.jpg',faces[0])\n",
    "        \n",
    "    image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\ashu5.jpg',cv2.IMREAD_COLOR)\n",
    "    faces_coord = detector.detect(image)\n",
    "    if(len(faces_coord)):\n",
    "        faces = cut_faces(image,faces_coord)\n",
    "        faces = normalize_intensity(faces)\n",
    "        faces = resize(faces)\n",
    "        cv2.imwrite(folder+'/ashu5_norm.jpg',faces[0])\n",
    "        \n",
    "    image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\ashu6.jpg',cv2.IMREAD_COLOR)\n",
    "    faces_coord = detector.detect(image)\n",
    "    if(len(faces_coord)):\n",
    "        faces = cut_faces(image,faces_coord)\n",
    "        faces = normalize_intensity(faces)\n",
    "        faces = resize(faces)\n",
    "        cv2.imwrite(folder+'/ashu6_norm.jpg',faces[0])\n",
    "        \n",
    "    image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\ashu7.jpg',cv2.IMREAD_COLOR)\n",
    "    faces_coord = detector.detect(image)\n",
    "    if(len(faces_coord)):\n",
    "        faces = cut_faces(image,faces_coord)\n",
    "        faces = normalize_intensity(faces)\n",
    "        faces = resize(faces)\n",
    "        cv2.imwrite(folder+'/ashu7_norm.jpg',faces[0])\n",
    "        \n",
    "    image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\ashu8.jpg',cv2.IMREAD_COLOR)\n",
    "    faces_coord = detector.detect(image)\n",
    "    if(len(faces_coord)):\n",
    "        faces = cut_faces(image,faces_coord)\n",
    "        faces = normalize_intensity(faces)\n",
    "        faces = resize(faces)\n",
    "        cv2.imwrite(folder+'/ashu8_norm.jpg',faces[0])\n",
    "        \n",
    "    image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\ashu9.jpg',cv2.IMREAD_COLOR)\n",
    "    faces_coord = detector.detect(image)\n",
    "    if(len(faces_coord)):\n",
    "        faces = cut_faces(image,faces_coord)\n",
    "        faces = normalize_intensity(faces)\n",
    "        faces = resize(faces)\n",
    "        cv2.imwrite(folder+'/ashu9_norm.jpg',faces[0])\n",
    "        \n",
    "    image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\ashu10.jpg',cv2.IMREAD_COLOR)\n",
    "    faces_coord = detector.detect(image)\n",
    "    if(len(faces_coord)):\n",
    "        faces = cut_faces(image,faces_coord)\n",
    "        faces = normalize_intensity(faces)\n",
    "        faces = resize(faces)\n",
    "        cv2.imwrite(folder+'/ashu10_norm.jpg',faces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dataset():\n",
    "    images = []\n",
    "    labels = []\n",
    "    labels_dict = {}\n",
    "    people = [person for person in os.listdir(\"people/\")]\n",
    "    for i,person in enumerate(people):\n",
    "        labels_dict[i] = person\n",
    "        for image in os.listdir(\"people/\"+person):\n",
    "            images.append(cv2.imread(\"people/\"+person+'/'+image,0))\n",
    "            labels.append(i)\n",
    "    return (images,np.array(labels),labels_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully.\n"
     ]
    }
   ],
   "source": [
    "images,labels,labels_dict = collect_dataset()\n",
    "\n",
    "# Eigen Face Recognizer algorithm.\n",
    "rec_eig = cv2.face.EigenFaceRecognizer_create()\n",
    "rec_eig.train(images,labels)\n",
    "\n",
    "# Fisher algorithm. Needs atleast two people data.\n",
    "rec_fisher = cv2.face.FisherFaceRecognizer_create()\n",
    "rec_fisher.train(images,labels)\n",
    "\n",
    "# LBPH algorithm\n",
    "rec_lbph = cv2.face.LBPHFaceRecognizer_create()\n",
    "rec_lbph.train(images,labels)\n",
    "\n",
    "print(\"Model trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to collect the sample image for Mr. Bean to test.\n",
    "image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\sample_test_bean.jpg',cv2.IMREAD_COLOR)\n",
    "faces_coord = detector.detect(image)\n",
    "if(len(faces_coord)):\n",
    "    faces = cut_faces(image,faces_coord)\n",
    "    faces = normalize_intensity(faces)\n",
    "    faces = resize(faces)\n",
    "    face = faces[0]\n",
    "    cv2.imwrite('test_converted_bean.jpg',faces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen Faces --> Prediction : Bean    Confidence : 2740\n",
      "Fisher Faces --> Prediction : Bean    Confidence : 576\n",
      "LBPH Faces --> Prediction : Bean    Confidence : 145\n"
     ]
    }
   ],
   "source": [
    "# Applyting given three algorithms to test dataset.\n",
    "\n",
    "#if cv2.__version__ >= \"3.1.0\":\n",
    "#    collector = cv2.face.StandardCollector_create()\n",
    "#    rec_eig.predict(face,collector)\n",
    "#    conf = collector.getDist()\n",
    "#    pred = collector.getLabel()\n",
    "#    print(\"Eigen Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))\n",
    "\n",
    "#    rec_fisher.predict(face,collector)\n",
    "#    conf = collector.getDist()\n",
    "#    pred = collector.getLabel()\n",
    "#    print(\"Fisher Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))\n",
    "\n",
    "#    rec_lbph.predict(face,collector)\n",
    "#    conf = collector.getDist()\n",
    "#    pred = collector.getLabel()\n",
    "#    print(\"LBPH Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))\n",
    "    \n",
    "#else:\n",
    "pred,conf = rec_eig.predict(face)\n",
    "print(\"Eigen Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))\n",
    "pred,conf = rec_fisher.predict(face)\n",
    "print(\"Fisher Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))\n",
    "pred,conf = rec_lbph.predict(face)\n",
    "print(\"LBPH Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to collect the sample image for Mr. Bean to test.\n",
    "image = cv2.imread('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\sample_test_ashu.jpg',cv2.IMREAD_COLOR)\n",
    "faces_coord = detector.detect(image)\n",
    "if(len(faces_coord)):\n",
    "    faces = cut_faces(image,faces_coord)\n",
    "    faces = normalize_intensity(faces)\n",
    "    faces = resize(faces)\n",
    "    face = faces[0]\n",
    "    cv2.imwrite('test_converted_ashu.jpg',faces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen Faces --> Prediction : Ashu    Confidence : 2767\n",
      "Fisher Faces --> Prediction : Ashu    Confidence : 35\n",
      "LBPH Faces --> Prediction : Ashu    Confidence : 149\n"
     ]
    }
   ],
   "source": [
    "# Applyting given three algorithms to test dataset.\n",
    "\n",
    "#if cv2.__version__ >= \"3.1.0\":\n",
    "#    collector = cv2.face.StandardCollector_create()\n",
    "#    rec_eig.predict(face,collector)\n",
    "#    conf = collector.getDist()\n",
    "#    pred = collector.getLabel()\n",
    "#    print(\"Eigen Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))\n",
    "\n",
    "#    rec_fisher.predict(face,collector)\n",
    "#    conf = collector.getDist()\n",
    "#    pred = collector.getLabel()\n",
    "#    print(\"Fisher Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))\n",
    "\n",
    "#    rec_lbph.predict(face,collector)\n",
    "#    conf = collector.getDist()\n",
    "#    pred = collector.getLabel()\n",
    "#    print(\"LBPH Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))\n",
    "    \n",
    "#else:\n",
    "pred,conf = rec_eig.predict(face)\n",
    "print(\"Eigen Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))\n",
    "pred,conf = rec_fisher.predict(face)\n",
    "print(\"Fisher Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))\n",
    "pred,conf = rec_lbph.predict(face)\n",
    "print(\"LBPH Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCamera2(object):\n",
    "    def __init__(self,index=0):\n",
    "        self.video = cv2.VideoCapture('C:\\\\Users\\\\Ashutosh\\\\Desktop\\\\Machine Learning Eckvation\\\\Sample ML\\\\FaceDetection\\\\beanv3.mp4')\n",
    "        self.index = index\n",
    "\n",
    "    def get_frame(self,in_grayscale=False):\n",
    "        _,frame = self.video.read()\n",
    "        if in_grayscale:\n",
    "            frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        return frame\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.video.release()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBPH Faces --> Prediction : Bean    Confidence : 142\n"
     ]
    }
   ],
   "source": [
    "# Detecting faces in a video.\n",
    "video1 = VideoCamera2()\n",
    "detector = FaceDetector(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "cv2.namedWindow(\"Face Detection Project\",cv2.WINDOW_AUTOSIZE);\n",
    "while True:\n",
    "    frame = video1.get_frame()\n",
    "    faces_coord = detector.detect(frame,True)\n",
    "    if(len(faces_coord)):\n",
    "        faces = cut_faces(frame,faces_coord)\n",
    "        faces = normalize_intensity(faces)\n",
    "        faces = resize(faces)\n",
    "        for i,face in enumerate(faces):\n",
    "            pred,conf = rec_lbph.predict(face)\n",
    "            threshold = 200\n",
    "            print(\"LBPH Faces --> Prediction : \"+labels_dict[pred].capitalize()+\"    Confidence : \"+str(round(conf)))\n",
    "            if(conf<threshold):\n",
    "                cv2.putText(frame,labels_dict[pred].capitalize(),(faces_coord[i][0],faces_coord[i][1]-10),cv2.FONT_HERSHEY_PLAIN,3,(66,53,243),2)\n",
    "            else:\n",
    "                cv2.putText(frame,\"Unknown\",(faces_coord[i][0],faces_coord[i][1]-10),cv2.FONT_HERSHEY_PLAIN,3,(66,53,243),2)\n",
    "        clear_output(wait=True)\n",
    "        for (x,y,w,h) in faces_coord:\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(150,150,0),8)\n",
    "    cv2.putText(frame,\"ESC to Exit\",(5,frame.shape[0]-5),cv2.FONT_HERSHEY_PLAIN,1.3,(66,53,243),2,cv2.LINE_AA)\n",
    "    cv2.imshow(\"Face Detection Project\",frame)\n",
    "    if(cv2.waitKey(40) & 0xFF==27):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
